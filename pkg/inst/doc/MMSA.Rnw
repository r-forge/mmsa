%\documentclass[fleqn, letter, 10pt]{article}
%\documentclass[article]{jss}
\documentclass[nojss]{jss}
%\usepackage[round,longnamesfirst]{natbib}
%\usepackage[left=1.5in,top=1.5in,right=1.5in,bottom=1.5in,nohead]{geometry} 
%\usepackage{graphicx,keyval,thumbpdf,url}
%\usepackage{hyperref}
%\usepackage{Sweave}
%\SweaveOpts{strip.white=TRUE, eps=FALSE}
%\AtBeginDocument{\setkeys{Gin}{width=0.6\textwidth}}


\usepackage[utf8]{inputenc}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsfonts}


%\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\func}[1]{\mbox{\texttt{#1()}}}
%\newcommand{\code}[1]{\mbox{\texttt{#1}}}
%\newcommand{\pkg}[1]{\strong{#1}}
%\newcommand{\samp}[1]{`\mbox{\texttt{#1}}'}
%\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
%\newcommand{\sQuote}[1]{`{#1}'}
%\newcommand{\dQuote}[1]{``{#1}''}
\newcommand\R{{\mathbb{R}}}

%\DeclareMathOperator*{\argmin}{argmin}
%\DeclareMathOperator*{\argmax}{argmax}

%\setlength{\parindent}{0mm}
%\setlength{\parskip}{3mm plus2mm minus2mm}

%% \VignetteIndexEntry{MMSA: Position Sensitive P-Mer Frequency Clustering with Applications to Genomic Classification and Differentiation}


\author{Anurag Nagar\\Southern Methodist University \And
    Michael Hahsler\\Southern Methodist University}
\title{\pkg{MMSA}: Position Sensitive P-Mer Frequency Clustering with Applications to Genomic Classification and Differentiation}
%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Anurag Nagar, Michael Hahsler} %% comma-separated
\Plaintitle{MMSA: Position Sensitive P-Mer Frequency Clustering with Applications to Genomic Classification and Differentiation} %% without formatting
\Shorttitle{Position Sensitive P-Mer Frequency Clustering} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel lossy alignment techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the development of fast and inexpensive devices for microbiome-based health screening in the near future.
}
\Keywords{data mining, clustering, Markov chain}
\Plainkeywords{data mining, clustering, Markov chain} %% without formatting

%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
    Anurag Nagar\\
        Computer Science and Engineering\\
        Lyle School of Engineering\\
        Southern Methodist University\\
        P.O. Box 750122 \\
        Dallas, TX 75275-0122\\
        E-mail: \email{anagar@smu.edu}\\
}
\Address{
    Michael Hahsler\\
        Computer Science and Engineering\\
        Lyle School of Engineering\\
        Southern Methodist University\\
        P.O. Box 750122 \\
        Dallas, TX 75275-0122\\
        E-mail: \email{mhahsler@lyle.smu.edu}\\
        URL: \url{http://lyle.smu.edu/~mhahsler}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
    %% need no \usepackage{Sweave.sty}

    %% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}

%\title{rEMM: Extensible Markov Model for Data Stream Clustering in R}
%\author{Michael Hahsler and Margaret H. Dunham}
%\date{}

%\maketitle
\sloppy

%\abstract{}

<<echo=FALSE>>=
options(width = 70, prompt="R> ", digits=4)
### for sampling
set.seed(1234)
@

\section{Introduction}
Recent advances in Metagenomics and the Human Microbiome provide a complex
landscape for dealing with a multitude of genomes all at once. One of the many
challenges in this field is classification of the genomes present in the
sample. Effective metagenomic classification and diversity analysis require
complex representations of taxa. In this package, we
develop a suite of tools, based on novel lossy alignment techniques that will
be applied to environmental metagenomics samples as well as human microbiome
samples. 

Markov Models in Sequence Analysis (MMSA) project used Markov Models for identification, organization, and classification of various species
based on their genetic sequences. More specifically, we use sequences for 16S rRNA gene, which is known to be conserved across species and is widely used
for identification and classification. Sequences for the 16S rRNA gene can be freely downloaded from the website of the Greengenes project ~\citep{MMSA:2012:Greengenes}.

\section{Using TRACDS for Genomic Applications}
The Extensible Markov Model (EMM) can be understood as an evolving Markov Chain (MC) model which is updated when new data is arrived. It has
several benefits over traditional Markov Models and can be used to create compact and space efficient models for data streams and other time-varying data.
The  R package rEMM \citep{MMSA:rEMM:2012}  implements a robust and stable implementation of EMM.

%%start
Sequence Clustering using Position Sensitive $p$-mer Clustering is based 
on the idea of computing distances between sequences using 
of $p$-mer frequency counts instead of 
computationally expensive alignment between the original sequences. 
This idea is at the core of so-called alignment-free 
methods~\cite{sequence:Vinga:2003}. 
However, in contrast to these methods we count $p$-mer frequencies 
position specific (i.e., for different segments of the sequence) and then use
high-throughput stream clustering to group similar segments. 
This approach completely
avoids expensive alignment of sequences prior to building the models. Even so,
because of the clustering of like sequence segments, a probabilistic local
quasi-alignment is automatically achieved. This quasi-alignment
is exploited for visualization.

The occurrences of letters or base compositions \{A, C, U, G\}
of an 16S rRNA sequence provide frequency information. The occurrences of all
patterns of bases of length $p$ generates a $p$-mer frequency representation
for a sequence.
Instead of global frequencies, we count $p$-mer frequencies locally to retain
positional information by moving a window of a given size $L$ across a sequence
starting at the first position. Within each window (called a segment) we count
the frequencies for all possible $p$-mers. We call this frequency profile a
Numerical Summarization Vector (NSV). For example, suppose we have an input
window containing ACGTGCACG. If counting 2-mers, the NSV count vector would be 
$\langle 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0\rangle$
representing counts of the subpatterns 
AA, AC, AG, AT, CA, CC, CG, CT, GA, GC,
GG, GT, TA, TC, TG, and TT. 
As the window slides down the input sequence, in
each new segment $p$-mers are counted. Window sizes may be varied, windows may
or may not overlap and different values for $p$ can be used within the same
sequences.

\begin{figure*}
\centering
\includegraphics[width=.85\linewidth]{TRACDS_model_building}
\caption{The Model Building Process. Numerical Summary Vectors (NSV) constitute
the numerical representations of segments (a moving window) along a sequence
which are used in building a cluster model. Model building starts with 
an empty cluster model. As each NSV is processed, it is compared to the 
existing clusters of the model.  If the NSV is not found to be close 
enough (using a distance measure on the NSVs) a new cluster is creates.
For example Cluster~1 (circle) is created for NSV~1, Cluster~2 for NSV~2 and
Cluster~3 for NSV~3. NSV~3 was found close enough to NSV~2 and thus was also
assigned to Cluster~1. In addition to the clusters also the transition
information between the clusters (arrows) is recorded.  When all NSVs are
processed, the model building process is finished.}
\label{emm}
\end{figure*}

%% FIXME: Algorithm here? 
Figure~\ref{emm} summarizes the model building process. 
NSVs are clustered using high-throughput data stream clustering 
techniques and the
sequence information for the NSVs is preserved in a directed graph $G=(N, E)$,
where $N={c_1,c_2,\ldots,c_N}$ is the set of clusters and 
$E={e_1,e_1,\ldots,e_E}$ is the set of transitions.
This graph can be interpreted as a Markov Chain, however, unlike a classical
Markov Model, each node is not bound to one symbol. In fact, each node
represents a cluster consisting of NSVs that are found to be similar by the
model building process according to a similarity or dissimilarity metric. Since
several NSVs (i.e., segments) can be assigned to the same cluster, the 
resulting
model compresses the original sequence (or sequences if several sequences are 
clustered into the same model). The directed edges are
associated with additional information representing the probabilities of
traversal assigned during the model building process. It is important to
observe that intra-similarity of a sequence is captured in nodes to the extent
possible based on the segment length $L$ and transitional information from one
segment to next is condensed to its relative probability within the sequence.
Since a segment is contiguous, its integrity is preserved in the model
inside nodes. Though the segments of a sequence may be distributed all over the
graph, the transition among them is still available as weighted probabilities
on the edges of the graph.


The Similarity between NSVs used for clustering 
can be calculated using several measures. Measures
suggested in the literature about alignment free methods to compare sequences
using $p$-mer counts are Euclidean distance, squared Euclidean distance,
Kullback-Leibler discrepancy and Mahalanobis
distance~\cite{sequence:Vinga:2003}.
Recently, for Simrank~\cite{sequence:DeSantis:2011} an even simpler similarity
measure, the number of matching $p$-mers (typically with $p=7$), was proposed
for efficient search of very large database.
In the area of approximate string matching Ukkonen proposed the 
approximate the expensive computation of the edit distance between two strings
by using $q$-grams (analog to $p$-mers in sequences). First
$q$-gram profiles are computed
and then the distance between the profiles is calculated using
Manhattan distance~\cite{sequence:Ukkonen:1992}.
The Manhattan distance between
two $p$-mer NSVs $x$ and $y$ is defined as:

$$d_\mathrm{Manhattan}(x,y)=\sum_{i=1}^{4^p} |x_i-y_i|$$

Manhattan distance also has a particularly straightforward
interpretation.
The distance counts the number of $p$-mers by which two sequences
differ which gives the following lower bound
on the edit distance between the original sequences $s_x$, $s_y$:
$$d_\mathrm{Manhattan}(x,y)/(2p) \le d_\mathrm{Edit}(s_x,s_y)$$

This relationship is easy to proof since each insertion/deletion/substitution 
in a sequences destroys at the most $p$ $p$-grams and introduces
at most $p$ new $p$-grams.
Although, we can construct two completely differnt sequences with exactly
the same NSVs (see \cite{sequence:Ukkonen:1992} for a method), we are typically
interested in sequences of high similarity in which case 
$d_\mathrm{Manhattan}(x,y)/(2p)$ gets closer to the edit distance.

Altough our approach can be used with any distance measure, we will
only report results using Manhattan distance in this paper.

A cluster model can be created for a single sequence and compresses the
sequence
information first by creating NSVs and then reduces the number of NSVs 
to the number of clusters 
needed to represent the whole sequence. Typically, we will create
a cluster model for a whole family of sequences by simply adding the NSVs
of all sequences to a single model following the procedure in Figure~\ref{emm}.
This will lead to even more compression since many sequences within a family
will share NSVs steming from similar sequence segments. We explored
this approach for taxnomic classification in~\cite{hahsler:Kotamarti:2010}. 
In the following section we will show how the same apporach can be used for
visualizing gentetic sequence databases.


%%end
\section{MMSA Package}
In order to start using MMSA, the following packages must be installed: seqinr, rEMM, proxy, cluster, MASS, clusterGeneration, iGraph, caTools, bitops, DBI, RSQLite.
MMSA builds on the above packages to develop efficient storage, querying, analysis, and plotting capabilities..
\subsection{Genetic Databases}
At the heart of the MMSA package are Genetic Databases (genDB) which are used for efficient storage and retrieval. The examples below use SQLite database, but any other
compatible database such as mySQL, or mSQL can also be used. We use the sequences from Greengenes and the associated sequence ID as the primary key in the tables. 

\begin{figure}[h]
\centering
\includegraphics[width=5in]{er-diagram}
\caption{Entity Relationship diagram of genetic database}
\label{fig:ER_Diag}
\end{figure}

\section{Examples}

First, we load the library into the R environment.
<<>>=
library(MMSA)
@

The first step is to create an empty genDB into which sequences can be loaded.
<<>>=
db<-createGenDB("example.sqlite")
@
The above command creates an empty database with table structure similar to Figure \ref{fig:ER_Diag}
and stores it in the file "example.sqlite".
If a genDB already exists, then it can be opened and loaded using the \func{openGenDB} command

The next step is to load data into the database by reading FASTA files.
Since the meta info in FASTA files is not standardized, we provide several reader functions. For example,
for FASTA files for GreenGenes, we provide the reader function \func{addSequencesGreengenes}, which will
automatically read and parse GreenGenes FASTA files and extract the classification and sequence information. 
For each sequence, the header information will be parsed and stored in the appropriate column of the ``Classification'' table. 
For example, the kingdom value will go into the kingdom field of the table and so on. Each sequence also has a unique ID. This will
be parsed and used as the \textit{Primary Key} in the ``Classification'' and ``Sequences'' tables. Similarly, the DNA sequence will be
stored in the ``Sequences'' table as a Binary Large Object (BLOB). As of now, we have implemented the reader for Greengenes format FASTA files, and
work is in progress to develop more readers for NCBI format FASTA files. 

The command below uses a FASTA file provided by the package, hence we use system.file instead of just a string with the file name.
<<>>= 
addSequencesGreengenes(db,
system.file("examples/phylums/Firmicutes.fasta",package="MMSA"))
@

After loading the sequences, various querying and limiting functions can be used to check the data and obtain 
a subset of the sequences.
The name of the genDB outputs the names of all the tables contained.
<<>>=
db
@
To get a count of the number of sequences in the database, the function \func{nSequences} can be used:
<<>>=
nSequences(db)
@

Similarly, the function \func{getSequences} returns the DNA sequences as a vector. The first 50 bases of the first sequence are shown as an example.
<<>>=
d<-getSequences(db)
substr(d[1],1,50)
@
The sequences can also be filtered at various ranks. For example, to get the sequences that have the genus name ``Desulfosporomusa'', the following
command is issued:
<<>>=
d<-getSequences(db,rank="genus",name="Desulfosporomusa")
@
It's very easy to obtain the various unique names stored in the database for any rank. For example, to find all names for the rank ``order'',
we use the \func{getRank}.
<<>>=
getRank(db,rank="order")
@
In the above case, the 100 sequences only contain organisms from \textit{two} different orders. 

If you would like to query more specific hierarchies, that's easily done as well. For example, if you want to find out the genuses within the phylum ``Firmicutes'',
you can use the function \func{getRank} with filters as:
<<>>=
getRank(db, rank="genus", whereRank="phylum", whereName="Firmicutes")
@
The above output shows all the genuses within the phylum ``Firmicutes''.

\subsection{Converting Sequences to NSV}
In order to create position sensitive p-mer clustering algorithms and models, we need to first create Numerical Summarization Vectors (NSVs).
The MMSA package can easily convert large number of sequences to NSV format and store them in the same database as character sequences. 
The following command will convert all the sequences to NSV format  and store them in a table called "NSV".
<<>>=
createNSVTable(db,tableName="NSV")
@
In the above function, we have accepted the default values of all the parameters such as word, overlap, and last\_window.
More parameters and filter criteria can also be easily specified as:
<<>>=
createNSVTable(db, tableName="NSV1", whereRank="genus",whereName="Thermosinus", 
window=100, overlap=0, word=3,last_window=FALSE)
@
The above command converts only those sequences that belong to the genus ``Thermosinus'' and stores them in a table called ``NSV1''.
The parameters for creating NSVs are also part of the command, such as window size is 100, overlap is 0, word size is 3, and
last\_window parameter is FALSE.

When a new NSV table is created, its name and meta information is stored in the metaData table. The meta data can be queried using the
\func{metaGenDB} function.
<<>>=
metaGenDB(db)["annotation"]
@

The sequences in the NSV tables can be queried and filtered just as easily. For example, we can get the NSV sequences using the function
\func{getSequences} as before:
<<>>=
dNSV<-getSequences(db, rank="Genus", name="Desulfosporomusa",table="NSV")
@

Most of the times, we would like sequences to be in NSV format in the database as the NSVs form the basis of most operations such as model generation,
and classification. The function \func{processSequences} loads all the FASTA sequences from a direct into the database and also converts them 
into NSVs. This function saves the extra step of loading sequences and then creating NSVs explicitly. 
<<>>=
closeGenDB(db)
unlink("example.sqlite")
db<-createGenDB("example.sqlite")
processSequences(system.file("examples/phylums",package="MMSA"),db)
@

In the above code, we have first closed and deleted the existing database. This is done using the function \func{closeGenDB}. Note that the ``sequences''
table does not allow duplicate sequence IDs to be stored. The above code shows how to add \textbf{all} the FASTA files from the system directory ``phylums'' that
is a part of the MMSA package by using the \func{processSequences}. The function also automatically converts the sequences to NSV format and stores them
in a table called ``NSV''.

\subsection{Creating Models}
The NSVs created in the previous section can be used in model generation. The models can be created at any level and for any set of subsequences or
the even the entire dataset of sequences. 
<<>>=
model<- genModel(db,rank="Genus",name="Desulfosporomusa",table="NSV", limit=100, 
measure="Kullback", threshold=0.1,plus_one=TRUE)
@
The above command creates a model using a subset of the sequences in the NSV table that belong to the genus ``Desulfosporomusa''. For creating the model,
we use ``Kullback'' similarity measure with a  threshold  equal to 0.1. The plus\_one parameter gives each transition an initial count of one. For more details
about model creation, please see the reference manual of the rEMM package \citep{MMSA:rEMM:2012}. The "limit" parameter limits the maximum number of sequences to
be used in model creation. The model is a compact signature of the sequences and can be easily and efficiently used for analysis. It can be plotted to get a visual
display of the various states and transitions, and can be used for classification by scoring new sequences against existing models. 
Often it is needed to create models at a particular rank level. For example, you might want to create models for each phylum present in the database. This can be easily accomplished using the \func{createModels}, which creates models and stores them in a directory  with the rank's name (such as ``phylum'').

The above command creates models for all phylums stored in the database and stores them in a directory called "phylum"
<<>>=
dir.create("models")
createModels(modelDir="models",rank="phylum",db)
@

The models are stored as compressed .rds files in the ``models/phylum'' directory. You can get a list of files stored in that directory by the \func{list.files} function 
<<>>=
list.files("models")
@
The models can be easily loaded using the readRDS function.
<<>>=
model<-readRDS("models/phylum/Firmicutes.rds")
@



\subsection{Classification}
Once the models have been constructed, they can be used to score and classify new sequences. We can compare the new sequence against just one model
or all the models stored in a directory. The classification score is based on the \func{score} from the rEMM package \citep{MMSA:rEMM:2012}
and evaluates how likely it is that a sequence was generated by a given model. It is calculated by the length-normalized 
product or sum of probabilities on the path along the new sequence.
The scores for a new sequence of length $l$ are defined as:
\begin{align}
P_\mathrm{prod} &= \sqrt[l-1]{\prod_{i=1}^{l-1}{a_{s(i)s(i+1)}}} \\
P_\mathrm{sum} &= \frac{1}{l-1} \sum_{i=1}^{l-1}{a_{s(i)s(i+1)}}
\end{align}

where $s(i)$ is the state of the $i^\textrm{th}$ data point in the new sequence it is assigned to. 
Points are assigned to the closest cluster only if the distance to the center is smaller
than the threshold. Data points which are not within the threshold of any cluster stay unassigned.
Note that for a sequence of length $l$ we have $l-1$ transitions. If we want to take the initial
transition probability also into account we extend the above equations by the additional initial probability $a_{\epsilon,s(1)}$:
\begin{align}
P_\mathrm{prod} &= \sqrt[l]{\prod_{i=1}^{l-1}{a_{s(i)s(i+1)}}} \\
P_\mathrm{sum} &= \frac{1}{l} \left( a_{\epsilon,s(1)}+\sum_{i=1}^{l-1}{a_{s(i)s(i+1)}} \right)
\end{align}

The \func{score.GenModel} uses the above equations to compute the similarity between a sequence and a model. 
<<>>=
score.GenModel(model,dNSV[[1]]+1)
@
The +1 used in the sequences is used to prevent zero count values in the sequences. This is necessary if the model is created using  the
``Kullback'' similarity measure. If it is created using any other method such as ``Manhattan'', this would not be necessary.

The function \func{classify} can be used to classify NSV sequences against all the models stored in a directory. It will output a data frame containing the
score matrix and the actual and predicted ranks. The following example shows how the \func{classify} works:
<<>>=
d<-getSequences(db,rank="phylum",table="NSV",limit=10,random=TRUE)
classification<-classify(modelDir="models",d)
head(classification$scores)
head(classification$prediction)
@


\subsection{Validation of Classification}

Often, we would like to use a subset of the entire sequences for training and the rest for test purposes. Such a functionality is also available in the 
MMSA package. The function \func{validateModels} splits up the sequences into \textit{training} and \textit{test} datasets and used the training sequences
for generating the models. The parameter \textit{pctTest} is used to specify the fraction of sequences to be used for test dataset.
The output is a data frame containing the classification results as before.
<<>>=
validation<-validateModels(db,modelDir="models",rank="phylum",table="NSV",
pctTest=0.1)
head(validation$scores)
head(validation$prediction)
@



\subsection{Visualization}
The EMM model emm created in the previous section can be easily visualized using the MMSA package. The following command plots the EMM model "emm":
<<EMM_1, fig=TRUE, include=FALSE>>=
plot.GenModel(model)
@

\begin{figure}[h]
\centering
\includegraphics[width=.5\linewidth]{MMSA-EMM_1}
\caption{Plot of EMM}
\label{fig:EMM_Plot}
\end{figure}




\section*{Acknowledgments}
This research is supported by research grant no. R21HG005912 from the National Human Genome Research Institute (NHGRI / NIH).

<<echo=FALSE>>=
unlink("example.sqlite")
system("rm -rf models/")
@

%\bibliographystyle{abbrvnat}
%\bibliography{MMSA}

\bibliography{MMSA}


\end{document}
